{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak3SqhmRDYKW"
      },
      "source": [
        "## CSCE 676 :: Data Mining and Analysis :: Texas A&M University :: Spring 2022\n",
        "\n",
        "\n",
        "# Homework 1: Let's GOOOOO!\n",
        "\n",
        "- **100 points [7% of your final grade]**\n",
        "- **Due Tuesday, February 13 by 11:59pm**\n",
        "\n",
        "***Goals of this homework:***\n",
        "1. Collect data from the web, clean it, and then make some observations based on exploratory data analysis\n",
        "2. Understand and implement the classic apriori algorithm and extensions to find the association rules in a movie rating dataset\n",
        "\n",
        "***Submission instructions:***\n",
        "\n",
        "You should post your notebook to Canvas (look for the homework 1 assignment there). Please name your submission **your-uin_hw1.ipynb**, so for example, my submission would be something like **555001234_hw1.ipynb**. Your notebook should be fully executed when you submit ... so run all the cells for us so we can see the output, then submit that. \n",
        "\n",
        "***Late Days:***\n",
        "\n",
        "As a reminder, you may use up to three of your late days on this homework, meaning the latest we will accept it is February 16 by 11:59pm.\n",
        "\n",
        "***Collaboration declaration:***\n",
        "\n",
        "If you worked with someone on this homework, please be sure to mention that. Remember to include citations to any sources you use in the homework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiUzZn-6Ecs8"
      },
      "source": [
        "## (50 points) Part 1: UFOs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHXUGi-gm88u"
      },
      "source": [
        "### (10pts) Part 1a: UFOs are Out There, But First I Need to Store them Locally\n",
        "\n",
        "For this first part, we're going to collect evidence of UFO sightings from the **National UFO Reporting Center**. Specifically, we're going \n",
        "to focus only on UFO sightings in Texas, as reported at this webpage:\n",
        "\n",
        "* http://www.nuforc.org/webreports/ndxlTX.html\n",
        "\n",
        "Recall that you can view the source of a webpage in Chrome under View &rarr; Developer &rarr; View Source. \n",
        "You'll notice, however, that this raw HTML is not in our friendly csv format and so will require some initial pre-processing. \n",
        "In particular, we're going to use the Python libraries **[requests](http://docs.python-requests.org/en/master/)** \n",
        "and **[beautiful soup](https://www.crummy.com/software/BeautifulSoup/)** to convert this UFO data from its original HTML format into csv. \n",
        "\n",
        "Hints:\n",
        "* You'll notice that the column headers are in the `<TH>` tags.\n",
        "* The values are in the `<TD>` tags.\n",
        "* In beautiful soup, something like `.find_all('td')` may help you.\n",
        "* To write the csv, you might want to `import csv` and take a look at the functions provided.\n",
        "* If you google for \"beautifulsoup table to csv\" you should find some nice starting points.  Note, however, that you may not use an existing method that auto-magically converts the HTML into csv; we expect you to write your own code. If you borrow some elements from online resources, you should cite them in the comments. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R7e6aYoQEYTT"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "# you should use requests to get the webpage, then extract \n",
        "# the appropriate column headings and rows\n",
        "# then write this out to csv to a local file called 'ufos_in_texas.csv'\n",
        "\"\"\"\n",
        "Sources:https://www.pluralsight.com/guides/extracting-data-html-beautifulsoup\n",
        "https://www.kite.com/python/examples/4420/beautifulsoup-parse-an-html-table-and-write-to-a-csv\n",
        "\n",
        "\"\"\"\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "# from itertools import izip\n",
        "import csv\n",
        "url ='http://www.nuforc.org/webreports/ndxlTX.html'\n",
        "webpage = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(webpage.text, \"html.parser\")\n",
        "ufo_table=soup.find(\"table\")\n",
        "#There is only one table in the entire webpage with no class\n",
        "\n",
        "headers=[]\n",
        "out_data=[]\n",
        "\n",
        "#Extracting Column Headings\n",
        "for col in ufo_table.findAll('th'):\n",
        "  headers.append(col.text)\n",
        "# print(headers)\n",
        "\n",
        "#Extracting the row-wise Data shared by Users\n",
        "for row in ufo_table.findAll('tr'):\n",
        "  iter_data={}\n",
        "  for iter in range(len(row.findAll(\"td\"))):\n",
        "    iter_data[headers[iter]]=row.findAll(\"td\")[iter].text\n",
        "  out_data.append(iter_data)\n",
        "# print(out_data)\n",
        "\n",
        "#Writing into csv file\n",
        "with open('ufos_in_texas.csv','w') as csvfile:\n",
        "  output=csv.DictWriter(csvfile, fieldnames=headers)\n",
        "  output.writeheader()\n",
        "  for x in out_data:\n",
        "    if x:\n",
        "      output.writerow(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfQTWXRfnEZN"
      },
      "source": [
        "Once you have your local csv file, you should read it in and then issue the .head() command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lGjL27fsnEIu",
        "outputId": "6ff71f61-4780-4cd9-c562-b498f850fef7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4f2cab7c-0e71-4105-96ba-7776b0f7356a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date / Time</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Shape</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Posted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12/14/21 22:30</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>TX</td>\n",
              "      <td>Formation</td>\n",
              "      <td>8 minutes</td>\n",
              "      <td>It was loud like rocket in a V shape.</td>\n",
              "      <td>12/19/21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12/12/21 17:30</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>TX</td>\n",
              "      <td>Light</td>\n",
              "      <td>10 minutes</td>\n",
              "      <td>A light that was fading in and out.</td>\n",
              "      <td>12/19/21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12/9/21 16:30</td>\n",
              "      <td>Lazerbet</td>\n",
              "      <td>TX</td>\n",
              "      <td>Other</td>\n",
              "      <td>Google maping</td>\n",
              "      <td>It's big</td>\n",
              "      <td>12/19/21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12/9/21 16:00</td>\n",
              "      <td>Lazerbet</td>\n",
              "      <td>TX</td>\n",
              "      <td>Triangle</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Unhuman</td>\n",
              "      <td>12/19/21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12/7/21 17:30</td>\n",
              "      <td>Oak Cliff</td>\n",
              "      <td>TX</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I was randomly taking pictures of the clouds a...</td>\n",
              "      <td>12/19/21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f2cab7c-0e71-4105-96ba-7776b0f7356a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f2cab7c-0e71-4105-96ba-7776b0f7356a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f2cab7c-0e71-4105-96ba-7776b0f7356a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Date / Time  ...    Posted\n",
              "0  12/14/21 22:30  ...  12/19/21\n",
              "1  12/12/21 17:30  ...  12/19/21\n",
              "2   12/9/21 16:30  ...  12/19/21\n",
              "3   12/9/21 16:00  ...  12/19/21\n",
              "4   12/7/21 17:30  ...  12/19/21\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# your code here\n",
        "import pandas as pd\n",
        "\n",
        "\"\"\"\n",
        "https://www.tutorialspoint.com/how-to-read-csv-file-in-python\n",
        "https://www.javatpoint.com/pandas-dataframe-head#:~:text=Pandas%20DataFrame.-,head(),a%20data%20frame%20or%20series.\n",
        "\"\"\"\n",
        "data_ufo = pd.read_csv(\"ufos_in_texas.csv\")     \n",
        "data_ufo.head() \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCSVPkNOnYfI"
      },
      "source": [
        "### (15pts) Part 1b: UFOs are a Mess! Time to Clean Up!\n",
        "\n",
        "Okay, now we move to the fun part -- making sense of this messy data. These UFO reports are user-generated with little input validation. As a result, you will notice lots of oddities. \n",
        "\n",
        "Let's begin by focusing on the **Duration** column. As a first pass, let's make a grossly simplifying assumption -- that the only valid data is any duration that is of the form:  \n",
        "\n",
        "* 1 second\n",
        "* 2 seconds\n",
        "* ...\n",
        "* 1 minute\n",
        "* 2 minutes\n",
        "* ...\n",
        "* 1 hour\n",
        "* 2 hours \n",
        "* ...\n",
        "* 1 day\n",
        "* 2 days \n",
        "* ...\n",
        "\n",
        "That is, we will only accept positive integers followed by a space, followed by a properly spelled unit. Every other entry is invalid. For example, that means these are all invalid durations:\n",
        "\n",
        "* 1s\n",
        "* 2 min.\n",
        "* 2-3 seconds\n",
        "* 10-15min\n",
        "* 1 minute+\n",
        "* 30 minutes and longer\n",
        "* about 1.5 minutes\n",
        "\n",
        "You may find the **pandas** library to be very helpful for this part. Create a new pandas dataframe that only includes sightings with these values, **where you convert all durations into seconds**. How many total rows are there in the original dataset? How many rows in your new 'validated' dataset? Report the basic statistics of the duration in your new 'validated' dataset (report maximum, minimum, mean, and standard deviation values of duration). At last, plot a boxplot of the duration (in seconds) in your 'validated' dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wMC-mq6wEnnY"
      },
      "outputs": [],
      "source": [
        "# your code here \n",
        "# filter our invalid durations\n",
        "# convert all valid durations to seconds\n",
        "import pandas as pd\n",
        "\n",
        "# filetring out the Invalid Duration\n",
        "\n",
        "output_list=[]\n",
        "for iter in data_ufo[\"Duration\"]:\n",
        "  temp=str(iter).split()\n",
        "  flag=0\n",
        "  flag2=0\n",
        "  for iter2 in temp:\n",
        "    if iter2.lower() not in ['seconds','second','hour','hours','minute','minutes','day','days','year',\"years\"]:\n",
        "      for i in iter2:\n",
        "        flag=2\n",
        "        if i not in '1234567890':\n",
        "          flag=1\n",
        "          break\n",
        "    else:\n",
        "      flag2+=1\n",
        "    if(flag==1):\n",
        "      break\n",
        "  if(flag==2 and flag2==1):\n",
        "        output_list.append(iter)\n",
        "# print(\"The length of validated Dataset is \"+ str(len(output_list)))\n",
        "\n",
        "##Conversion to seconds\n",
        "seconds_list=[]\n",
        "temp=[]\n",
        "temps=[]\n",
        "for iter in output_list:\n",
        "  temp.append(iter.split()[1])\n",
        "  temps.append(iter.split()[0])\n",
        "for iter in output_list:\n",
        "  if(iter.split()[1].lower() in ['minutes','minutes']):\n",
        "    seconds_list.append(int(iter.split()[0])*60)\n",
        "  elif (iter.split()[1].lower() in ['hours','hour']):\n",
        "    seconds_list.append(int(iter.split()[0])*60*60)\n",
        "  elif (iter.split()[1].lower() in ['day','days']):\n",
        "    seconds_list.append(int(iter.split()[0])*24*60*60)\n",
        "  elif (iter.split()[1].lower() in ['years','year']):\n",
        "    seconds_list.append(int(iter.split()[0])*24*60*60*365)\n",
        "  else:\n",
        "    seconds_list.append(int(iter.split()[0]))\n",
        "# print(seconds_list)\n",
        "# print(set(temp))\n",
        "# print(set(temps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILgQfoRmJh7A"
      },
      "outputs": [],
      "source": [
        "# #Approach-2 using regex\n",
        "import re\n",
        "\n",
        "\"\"\"\n",
        "Collaborated with classmates for the idea of regex\n",
        "\"\"\"\n",
        "\n",
        "valid_unit = '^(\\d+\\s+(days|day|hours|hour|minutes|minute|seconds|second|week|weeks|month|months|year|years))$'\n",
        "ufo_data_clean = data_ufo.loc[data_ufo['Duration'].str.match(valid_unit, na = False, flags = re.I)]\n",
        "len(ufo_data_clean)\n",
        "\n",
        "seconds_list = []\n",
        "count = 0\n",
        "for i, j in ufo_data_clean.iterrows():\n",
        "  j = ufo_data_clean['Duration'][i]\n",
        "  d = j.lower().split()\n",
        "  time = int(d[0])\n",
        "  \n",
        " ## Conversion into seconds\n",
        "  if(d[1] == \"minute\" or d[1] == \"minutes\"):\n",
        "    time = time*60\n",
        "  elif(d[1] == \"hour\" or d[1] == \"hours\"):\n",
        "    time = time*60*60\n",
        "  elif(d[1] == \"day\" or d[1] == \"days\"):\n",
        "    time = time*24*60*60\n",
        "  elif(d[1] == \"week\" or d[1] == \"weeks\"):\n",
        "    time = time*7*24*60*60\n",
        "  elif(d[1] == \"month\" or d[1] == \"months\"):\n",
        "    time = time*30*24*60*60\n",
        "  elif(d[1] == \"year\" or d[1] == \"years\"):\n",
        "    time = time*365*24*60*60\n",
        "  seconds_list.append(time)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yio0HTHncIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5533f3-17db-4e5e-966d-4e3532e63548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in original dataset-> 5631\n",
            "The length of validated Dataset -> 2956\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# total rows in original dataset\n",
        "print('Total rows in original dataset-> '+ str(len(data_ufo)))\n",
        "# valid rows in your new 'validated' dataset\n",
        "print(\"The length of validated Dataset -> \"+ str(len(seconds_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "5VSAJpk6nd6f",
        "outputId": "9b129405-0941-42f5-99d9-bac7b7f880fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics of the Validated Set:\n",
            "Length ->2956\n",
            "Maximum ->157680000\n",
            "Minimum ->1\n",
            "Mean ->54919.65257104195\n",
            "Standard Deviation ->2900194.783316995\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# your code here\n",
        "# report the basic statistics of duration in 'validated' dataset\n",
        "import matplotlib.pyplot as plt\n",
        "seconds_list=pd.Series(seconds_list)\n",
        "print(\"Statistics of the Validated Set:\")\n",
        "print(\"Length ->\"+str(len(seconds_list)))\n",
        "print(\"Maximum ->\"+str(max(seconds_list)))\n",
        "print(\"Minimum ->\"+str(min(seconds_list)))\n",
        "print(\"Mean ->\"+str(seconds_list.mean()))\n",
        "print(\"Standard Deviation ->\"+str(seconds_list.std()))\n",
        "# boxplot code here\n",
        "plot = plt.subplot()\n",
        "plot.boxplot(seconds_list)\n",
        "plot.set_yscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRED23qlni_z"
      },
      "source": [
        "### (15pts) Part 1c: Can we do better?\n",
        "Interesting. But we threw away a **lot** of data. We can do better. For this part, you will do your best to clean up the durations from your original dataset. Keep in mind some initial guidelines:\n",
        "\n",
        "* If a duration has a range, use the average as its value. For example, if the duration is listed as “6-8 minutes”, you should consider the duration as “7 minutes”. (Again, you will need to eventually convert minutes into seconds).\n",
        "* If a duration has a “<” sign, you should simply ignore the “<” sign. For example if the duration is specified as “< 1 minute”, consider the duration to be “1 minute”. You should subsequently convert “1 minute” to \"60 seconds\".\n",
        "* If a duration has a “>” sign, you should simply ignore the “>” sign. \n",
        "* You should ignore any row with an empty duration.\n",
        "\n",
        "You will probably have to improvise as you go along, so **make detailed notes of what decisions you are making and why**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3zORgUTnmZg"
      },
      "outputs": [],
      "source": [
        "# your code here \n",
        "# clean data \n",
        "# convert cleaned durations to seconds\n",
        "#Without Regex\n",
        "\n",
        "\"\"\"Taking the following attributes\n",
        "1. Duration listed like 6-8 minutes or 6-8 minute \n",
        "2. Duration with > \n",
        "3. Duration with <\n",
        "\n",
        "only proper time units are considered like minute or minutes. \n",
        "min, hr have not been considered.\n",
        "\"\"\"\n",
        "\n",
        "compare=[]\n",
        "import pandas as pd\n",
        "output_list=[]\n",
        "for iter in data_ufo[\"Duration\"]:\n",
        "  iterq=iter\n",
        "  iter=str(iter)\n",
        "\n",
        "  iter=(iter).replace(\">\",\"\")\n",
        "  iter=(iter).replace(\"<\",\"\")\n",
        "  if(\"-\" in iter ):\n",
        "    if(len(iter.split(\"-\")[1].split())==2):\n",
        "      if(iter.split(\"-\")[1].split()[1].lower() in ['seconds','second','hour','hours','minute','minutes','day','days'] and iter.split(\"-\")[0].isnumeric() and iter.split(\"-\")[1].split()[0].isnumeric() ):\n",
        "        # print(iter)\n",
        "        iter=str((int(iter.split(\"-\")[0])+int(iter.split(\"-\")[1].split()[0]))//2)+\" \"+iter.split(\"-\")[1].split()[1]\n",
        "        compare.append(iterq)\n",
        "  temp=str(iter).split()\n",
        "  flag=0\n",
        "  flag2=0\n",
        "  for iter2 in temp:\n",
        "    if iter2.lower() not in ['seconds','second','hour','hours','minute','minutes','day','days','year','years']:\n",
        "      for i in iter2:\n",
        "        flag=2\n",
        "        if i not in '1234567890':\n",
        "          flag=1\n",
        "          break\n",
        "    else:\n",
        "      flag2+=1\n",
        "    if(flag==1):\n",
        "      break\n",
        "  if(flag==2 and flag2==1):\n",
        "        output_list.append(iter)\n",
        "# print(len(output_list))\n",
        "seconds_list=[]\n",
        "temp=[]\n",
        "temps=[]\n",
        "\n",
        "for iter in output_list:\n",
        "  temp.append(iter.split()[1])\n",
        "  temps.append(iter.split()[0])\n",
        "\n",
        "for iter in output_list:\n",
        "  if(iter.split()[1].lower() in ['minutes','minutes']):\n",
        "    seconds_list.append(int(iter.split()[0])*60)\n",
        "  elif (iter.split()[1].lower() in ['hours','hour']):\n",
        "    seconds_list.append(int(iter.split()[0])*60*60)\n",
        "  elif (iter.split()[1].lower() in ['day','days']):\n",
        "    seconds_list.append(int(iter.split()[0])*24*60*60)\n",
        "  elif (iter.split()[1].lower() in ['years','year']):\n",
        "    seconds_list.append(int(iter.split()[0])*24*60*60*365)\n",
        "  else:\n",
        "    seconds_list.append(int(iter.split()[0]))\n",
        "# print(seconds_list)\n",
        "# print(set(temp))\n",
        "# print(set(temps))\n",
        "\n",
        "output_compare=[]\n",
        "for iter in data_ufo[\"Duration\"]:\n",
        "  iter=str(iter)\n",
        "  if( \"-\" in iter):\n",
        "    output_compare.append(iter)\n",
        "\n",
        "# for i in output_compare:\n",
        "#   if(\">\" in i or \"<\" in i):\n",
        "#     print(i)\n",
        "# print(len(output_compare))\n",
        "# print(len(compare))\n",
        "# print(len(set(output_compare)))\n",
        "# print(len(set(compare)))\n",
        "# print((set(output_compare)-set(compare)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Regex-2\n",
        "# Collaborated with classmates for this approach\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\"\"\"Taking the following attributes\n",
        "1. Duration listed like 6-8 minutes or 6-8 minute \n",
        "2. Duration with > and with unit minute or minutes\n",
        "3. Duration with < and with unit minute or minutes\n",
        "4. 1. Duration listed like 6(space)-(space)8 minutes or 6 - 8 minute \n",
        "only proper time units are considered like minute or minutes. \n",
        "min, hr have not been considered.\n",
        "\"\"\"\n",
        "valid_1= '^(\\d+\\s+(?i)(second|seconds|minutes|minute|hour|hours|day|days))$' + '|' + '^(>\\d+\\s+(?i)(second|seconds|minutes|minute|hour|hours|day|days))$' + '|' + '^(<\\d+\\s+(?i)(second|seconds|minutes|minute|hour|hours|day|days))$' + '|' + '^(\\d+\\s*\\-\\s*\\d+\\s+(?i)(second|seconds|minutes|minute|hour|hours|day|days))$'\n",
        "\n",
        "# valid_1= '^(\\d+\\s+(?i)(second|seconds|minutes|minute|hour|hours|day|days|month|months|year|years))$' + '|' + '^((\\\\<|\\\\>|\\~)\\s*\\d+\\s*(?i)(second|seconds|minutes|minute|hour|hours|day|days|month|months|year|years))$' + '|' + '^(\\d+\\s*\\-\\s*\\d+\\s+(?i)(second|seconds|minutes|minute|hour|hours|day|days|month|months|year|years))$'\n",
        "data_ufo_valid = data_ufo.loc[data_ufo['Duration'].str.match(valid_1, na = False)]\n",
        "\n",
        "\n",
        "seconds_list_2 = []\n",
        "\n",
        "for i, dat in data_ufo_valid.iterrows():\n",
        "  dat = data_ufo_valid['Duration'][i]\n",
        "  dat = dat.lower()\n",
        "  value = re.findall('[0-9]+', dat)  \n",
        "  numeric = [int(i) for i in value] \n",
        "  time_1  = np.mean(numeric)\n",
        "  \n",
        "\n",
        "  if(dat.find(\"min\") != -1):\n",
        "    time_1 = time_1*60\n",
        "  elif(dat.find(\"hour\") != -1):\n",
        "    time_1 = time_1*60*60\n",
        "  elif(dat.find(\"day\") != -1):\n",
        "    time_1 = time_1*24*60*60\n",
        "  elif(dat.find(\"week\") != -1):\n",
        "    time_1 = time_1*7*24*60*60\n",
        "  elif(dat.find(\"month\") != -1):\n",
        "    time_1 = time_1*30*24*60*80\n",
        "  elif(dat.find(\"year\") != -1):\n",
        "    time_1 = time_1*365*24*60*60\n",
        "  seconds_list_2.append(time_1)\n",
        "\n"
      ],
      "metadata": {
        "id": "kPJDtD4tYFBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2PDFUk1nqY4",
        "outputId": "3c929148-a23b-444a-d047-841afc09a3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in original dataset- 5631\n",
            "Total rows in cleaned dataset - 3426\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# total rows in original dataset\n",
        "# valid rows in your cleaned dataset\n",
        "print('Total rows in original dataset- '+ str(len(data_ufo)))\n",
        "print('Total rows in cleaned dataset - '+str(len(seconds_list_2))) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "L_r7ogIHnquU",
        "outputId": "7d347961-768f-435b-8b05-9d82540fb48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum-> 157680000\n",
            "Minimum-> 1\n",
            "Mean-> 47524.17752347418\n",
            "Standard Deviation->2701016.9396190355\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# your code here\n",
        "# report the basic statistics of duration in your cleaned dataset\n",
        "# draw a boxplot for your cleaned dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "seconds_list=pd.Series(seconds_list)\n",
        "print(\"Maximum-> \"+ str(max(seconds_list)))\n",
        "print(\"Minimum-> \"+ str(min(seconds_list)))\n",
        "print(\"Mean-> \"+ str(seconds_list.mean()))\n",
        "print(\"Standard Deviation->\"+str(seconds_list.std()))\n",
        "\n",
        "plot_2 = plt.subplot()\n",
        "plot_2.boxplot(seconds_list_2)\n",
        "plot_2.set_yscale('log')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5rKvm1k7HHk"
      },
      "source": [
        "### (5pts) Part 1d: Observations and Conclusions\n",
        "\n",
        "Based on your analysis on part 1b and 1c, what observations or conclusions can you make from the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrnseH_t7RzX"
      },
      "source": [
        "*your answer here*\n",
        "\n",
        " \n",
        "\n",
        "1.  By filtering out all the data stringently, we miss out on a lot of valid data like in 1.b which has been captured in 1.c. \n",
        "2.   As there is more data, in the case of 1.c it is more beneficial to run the statistics and makes more sense than 1.b\n",
        "\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-n9Zd2j7uFC"
      },
      "source": [
        "### (5pts) Part 1e: Next Steps\n",
        "\n",
        "Now is your chance to conduct an interesting analysis on the UFO data you have collected. This is open-ended, so you may choose whatever direction you like. For example, you might want to take a look at the shape of the UFOs or perhaps the temporal aspects of the reports. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GngkwXtD8Kvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9f4047-3ec6-4ab1-87f3-be8fa65c8346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Common Shapes in given dataset is [('Light', 1136), ('Triangle', 560), ('Circle', 520), ('Other', 440), ('Unknown', 427), ('Disk', 344), ('Sphere', 342), ('Fireball', 302), ('Oval', 299), ('Formation', 188)]\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "import statistics\n",
        "from statistics import mode\n",
        "from collections import Counter\n",
        "\n",
        "shapes_ufo= data_ufo[\"Shape\"]\n",
        "c=Counter(shapes_ufo)\n",
        "i=10\n",
        "print(\"Top \"+str(i) + \" Common Shapes in given dataset is \" + str(c.most_common(i)))\n",
        "#Reference: https://www.codegrepper.com/code-examples/python/python+list+10+most+frequent+elemens+in+list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb6tGIBG8M2q"
      },
      "source": [
        "*tell us what next steps you took, and what you discovered*\n",
        "\n",
        "---\n",
        "\n",
        "The have made a list of the top 10 shapes in the dataset using **Counter**\n",
        "The Top 10 shapes are as follows:\n",
        "1. Light\n",
        "2. Traingle\n",
        "3. Circle\n",
        "4. Other\n",
        "5. Unknown\n",
        "6. Disk\n",
        "7. Sphere\n",
        "8. Fireball\n",
        "9. Oval\n",
        "10. Formation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ6FjJ2NEoFS"
      },
      "source": [
        "## (50 points) Part 2: Association Rules in Movie Rating Behaviors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKVTACJGFGYw"
      },
      "source": [
        "For the second part of this homework, we're going to examine movies using our understanding of association rules, to find movies that \"go together\". For this part, you will implement the apriori algorithm, and apply it to a movie rating dataset. We'll use the [MovieLens](https://grouplens.org/datasets/movielens/) dataset.\n",
        "\n",
        "First, run the next cell to load the dataset we are going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcZSAoCAFDyA",
        "outputId": "b433e717-b2cc-4976-9527-f9576d5bbcc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        }
      ],
      "source": [
        "import urllib3\n",
        "import zipfile\n",
        "\n",
        "http = urllib3.PoolManager()\n",
        "req = http.request(\"GET\", \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\", preload_content=False)\n",
        "\n",
        "with open(\"movie.zip\", 'wb') as out:\n",
        "  while True:\n",
        "    data = req.read(4096)\n",
        "    if not data:\n",
        "      break\n",
        "    out.write(data)\n",
        "req.release_conn()\n",
        "\n",
        "zFile = zipfile.ZipFile(\"movie.zip\", \"r\")\n",
        "for fileM in zFile.namelist():\n",
        "  zFile.extract(fileM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgIIGQ3lIgRH",
        "outputId": "dd076096-5cff-43c6-ffb3-aab44f21e0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "links.csv  movies.csv  ratings.csv  README.txt\ttags.csv\n"
          ]
        }
      ],
      "source": [
        "!ls ml-latest-small/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiF1Gc0q7qzj"
      },
      "source": [
        "In this dataset, there are four columns: `userId` is the integer ids of users, `movieId` is the integer ids of movies, `rating` is the rate of the user gives to the movie, and `timestamp` which we do not use here. Each row denotes that the user of given `userId` rated the movie of the given `movieId`. We are going to treat each user as a \"basket\", so you will need to collect all the movies that have been rated by a single user as a basket. \n",
        "\n",
        "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user rating behaviors where:\n",
        "\n",
        "1. Define `rating` >= 3 is \"like\" (that is, only consider movie ratings of 3 or higher in your baskets; you may ignore all others)\n",
        "2. `minsup` == 40 (out of 600 users/baskets); we may adjust this based on the discussion on Campuswire\n",
        "3. `minconf` == to be determined by a discussion on Campuswire. You may try several different choices, but we will converge on a good choice for everyone for the final submission.\n",
        " \n",
        "We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach. Do not copy-paste any existing code. We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing. Furthermore, you should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will highlight the method. \n",
        "\n",
        "To help get you started, we can load the ratings with the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y8yZnEVI3Oy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# read user ratings\n",
        "allRatings = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
        "allRatings = allRatings[allRatings['rating']>=3]\n",
        "df_movie_rating = allRatings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfwWA4d2Ne6r"
      },
      "source": [
        "### (15pts) Step 1: Implement Apriori Algorithm\n",
        "In this section, you need to implement the Apriori algorithm, we will check the correctness of your code and we encourage efficient implementation and skills of pruning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48ITOQG09fT2",
        "outputId": "b7e369a2-fdbf-42ad-a7ff-2d9a3487ba5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Group the data on User Ids\n",
            "0      [2542, 2987, 1954, 1500, 2094, 2268, 2654, 70,...\n",
            "1      [46970, 131724, 6874, 109487, 333, 122882, 715...\n",
            "2      [26409, 5181, 7899, 5764, 1275, 70946, 2288, 5...\n",
            "3      [3317, 1213, 4021, 1203, 595, 2874, 2762, 1077...\n",
            "4      [364, 1, 36, 410, 110, 150, 39, 253, 261, 608,...\n",
            "                             ...                        \n",
            "604    [7010, 2474, 40955, 1650, 1912, 1320, 1380, 45...\n",
            "605    [1544, 919, 2000, 589, 3994, 480, 3386, 1258, ...\n",
            "606    [2353, 6059, 750, 2329, 736, 4873, 4016, 8810,...\n",
            "607    [110, 589, 329, 1056, 786, 457, 434, 185, 356,...\n",
            "608    [159, 7367, 4232, 112556, 107436, 3623, 1089, ...\n",
            "Name: movieId, Length: 609, dtype: object\n",
            "\n",
            "\n",
            "Total of  8452 items.\n",
            "\n",
            "1 Itemsets with support > 150 are 37\n",
            "\n",
            "\n",
            "2 Itemsets with support > 150 are 30\n",
            "\n",
            "\n",
            "3 Itemsets with support > 150 are 2\n",
            "\n",
            "{1: {frozenset({32}), frozenset({1}), frozenset({380}), frozenset({150}), frozenset({1196}), frozenset({2959}), frozenset({480}), frozenset({7153}), frozenset({1270}), frozenset({1210}), frozenset({4993}), frozenset({3578}), frozenset({1198}), frozenset({4306}), frozenset({2858}), frozenset({780}), frozenset({457}), frozenset({589}), frozenset({47}), frozenset({858}), frozenset({260}), frozenset({593}), frozenset({5952}), frozenset({588}), frozenset({592}), frozenset({364}), frozenset({318}), frozenset({2028}), frozenset({356}), frozenset({296}), frozenset({50}), frozenset({590}), frozenset({110}), frozenset({527}), frozenset({2762}), frozenset({608}), frozenset({2571})}, 2: {frozenset({296, 356}), frozenset({356, 589}), frozenset({110, 318}), frozenset({7153, 4993}), frozenset({2571, 1196}), frozenset({480, 356}), frozenset({1196, 260}), frozenset({50, 318}), frozenset({593, 318}), frozenset({593, 356}), frozenset({356, 110}), frozenset({296, 2571}), frozenset({1210, 260}), frozenset({318, 527}), frozenset({318, 2959}), frozenset({5952, 4993}), frozenset({296, 318}), frozenset({2571, 2959}), frozenset({2571, 318}), frozenset({1210, 1196}), frozenset({296, 47}), frozenset({5952, 7153}), frozenset({356, 150}), frozenset({2571, 356}), frozenset({296, 50}), frozenset({356, 318}), frozenset({356, 527}), frozenset({296, 593}), frozenset({296, 110}), frozenset({2571, 260})}, 3: {frozenset({296, 356, 318}), frozenset({296, 593, 318})}}\n",
            "There are total 14 Association rules for the given dataset\n",
            "\n",
            "[[[7153], [4993]], [[4993], [7153]], [[480], [356]], [[1196], [260]], [[1210], [260]], [[5952], [4993]], [[4993], [5952]], [[1210], [1196]], [[47], [296]], [[5952], [7153]], [[7153], [5952]], [[150], [356]], [[296, 593], [318]], [[593, 318], [296]]]\n",
            "Frequent itemset size is : 69\n"
          ]
        }
      ],
      "source": [
        "# your code here, including all the helpful print statements\n",
        "from itertools import chain\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import urllib3\n",
        "import zipfile\n",
        "\n",
        "#Mapping all data with the corresponding UserIds\n",
        "def mappingdata(value):\n",
        "  print(\"Group the data on User Ids\")\n",
        "  groupId= df_movie_rating.sample(frac = value).groupby('userId')\n",
        "  users=groupId.groups.keys()\n",
        "  Id=[]\n",
        "  for i in users:\n",
        "      Id.append((groupId.get_group(i).movieId.values))\n",
        "  \n",
        "  #Updated Dataframe\n",
        "  Main_List=pd.DataFrame({'userId':users,'movieId':Id}).movieId\n",
        "  print(Main_List)\n",
        "  print()\n",
        "  return Main_List\n",
        "\n",
        "\n",
        "#Function for Apriori Algo\n",
        "def apriori(movie_list, min_Sup, min_Conf,alpha, minsup_DFactor):\n",
        "    total_Frequency = []\n",
        "    iter_Set = set()\n",
        "    for i in movie_list:\n",
        "        for j in i:\n",
        "             iter_Set.add(frozenset([j]))\n",
        "    print(\"\\nTotal of  \"+str(len(iter_Set))+ \" items.\")  \n",
        "\n",
        "\n",
        "    # Frequent ItemSet\n",
        "    Freq_Set = dict()\n",
        "\n",
        "\n",
        "    # Including support count\n",
        "    Items_support = defaultdict(int)\n",
        "\n",
        "\n",
        "    #Find first freq item Q\n",
        "    Q_ISet = set()\n",
        "    x = defaultdict(int)\n",
        "    for i in iter_Set:\n",
        "        for iter_Set in movie_list:\n",
        "            if i.issubset(iter_Set):\n",
        "                Items_support[i] += 1\n",
        "                x[i] += 1\n",
        "    for i, num in x.items():\n",
        "        support = num\n",
        "        if(support >= min_Sup*minsup_DFactor*alpha):\n",
        "          Q_ISet.add(i)\n",
        "    \n",
        "    curr_Freq_Set = Q_ISet\n",
        "    total_Frequency = total_Frequency + list(curr_Freq_Set)\n",
        "    \n",
        "    key = 2\n",
        "\n",
        "    # Calculating Freq itemsets\n",
        "    while(curr_Freq_Set):\n",
        "        x = len(curr_Freq_Set)\n",
        "        print(\"\\n\"+str(key-1)+\" size Itemsets for support > \" +str(min_Sup*alpha*minsup_DFactor)+\" are \"+str(x)+\"\\n\")\n",
        "        Freq_Set[key-1] = curr_Freq_Set\n",
        "\n",
        "\n",
        "        # Including the current item to freq items group\n",
        "        curr_Cand = set([i.union(j) for i in curr_Freq_Set for j in curr_Freq_Set if len(i.union(j)) == key])\n",
        "\n",
        "\n",
        "        #Pruning to remove the less freq itemsets\n",
        "        x = curr_Cand.copy()\n",
        "        for i in curr_Cand:\n",
        "            groups = combinations(i, key-1)\n",
        "            for s in groups:\n",
        "                if(frozenset(s) not in curr_Freq_Set):\n",
        "                    x.remove(i)\n",
        "                    break\n",
        "        curr_Cand=x\n",
        "\n",
        "        #To Swap the curr itemset with new itemset\n",
        "        Q_ISet = set()\n",
        "        x = defaultdict(int)\n",
        "        for i in curr_Cand:\n",
        "            for curr_Cand in movie_list:\n",
        "                if i.issubset(curr_Cand):\n",
        "                    Items_support[i] += 1\n",
        "                    x[i] += 1\n",
        "        for i, count in x.items():\n",
        "            sup = count\n",
        "            if(sup >= min_Sup*minsup_DFactor*alpha):\n",
        "              Q_ISet.add(i)\n",
        "\n",
        "        curr_Freq_Set=Q_ISet\n",
        "        total_Frequency = total_Frequency + list(curr_Freq_Set)\n",
        "        key += 1\n",
        "    \n",
        "    #Association Rules\n",
        "    \n",
        "    association_rules = []\n",
        "    for a, i in Freq_Set.items():\n",
        "        for j in i:\n",
        "            groups = chain.from_iterable(combinations(j, l) for l in range(1, len(j)))\n",
        "            for s in groups:\n",
        "                confidence = float(Items_support[j] / Items_support[frozenset(s)])\n",
        "                if(confidence >= min_Conf):\n",
        "                  association_rules.append([list(s) , list(j.difference(s))])\n",
        "    print(Freq_Set)\n",
        "    return association_rules, Freq_Set\n",
        "    \n",
        "\n",
        "association_rules, total_Frequency = apriori(mappingdata(1), 150, 0.8,1,1)\n",
        "print(\"There are total \"+str(len(association_rules))+\" Association rules for the given dataset\\n\")\n",
        "print(association_rules)\n",
        "\n",
        "\n",
        "total_movie_List = []\n",
        "for i in list(total_Frequency.values()):\n",
        "  total_movie_List = total_movie_List + list(i)\n",
        "\n",
        "print(\"Frequent itemset size is : \" + str(len(total_movie_List)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea6GbdOOBZOY"
      },
      "source": [
        "### (5pts) Step 2: Print Your Association Rules\n",
        "\n",
        "Next you should print your final association rules in the following format:\n",
        "\n",
        "**movie_name_1, movie_name_2, ... --> \n",
        "movie_name_k**\n",
        "\n",
        "where the movie names can be fetched by joining the movieId with the file `movies.csv`. For example, one rule that you might find is:\n",
        "\n",
        "**Matrix, The (1999),  Star Wars: Episode V - The Empire Strikes Back (1980),  Star Wars: Episode IV - A New Hope (1977),  -> \n",
        "Star Wars: Episode VI - Return of the Jedi (1983)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgh-9bt5hlFJ",
        "outputId": "1e7cc7af-3418-4c62-e2f4-a35ca4b056c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSbhiIkw9kj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe0d8006-157a-4c52-f53c-fe6215c92d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 14 Association rules and they are as follows \n",
            "\n",
            "['Lord of the Rings: The Return of the King, The (2003)']--> ['Lord of the Rings: The Fellowship of the Ring, The (2001)']\n",
            "['Lord of the Rings: The Fellowship of the Ring, The (2001)']--> ['Lord of the Rings: The Return of the King, The (2003)']\n",
            "['Jurassic Park (1993)']--> ['Forrest Gump (1994)']\n",
            "['Star Wars: Episode V - The Empire Strikes Back (1980)']--> ['Star Wars: Episode IV - A New Hope (1977)']\n",
            "['Star Wars: Episode VI - Return of the Jedi (1983)']--> ['Star Wars: Episode IV - A New Hope (1977)']\n",
            "['Lord of the Rings: The Two Towers, The (2002)']--> ['Lord of the Rings: The Fellowship of the Ring, The (2001)']\n",
            "['Lord of the Rings: The Fellowship of the Ring, The (2001)']--> ['Lord of the Rings: The Two Towers, The (2002)']\n",
            "['Star Wars: Episode VI - Return of the Jedi (1983)']--> ['Star Wars: Episode V - The Empire Strikes Back (1980)']\n",
            "['Seven (a.k.a. Se7en) (1995)']--> ['Pulp Fiction (1994)']\n",
            "['Lord of the Rings: The Two Towers, The (2002)']--> ['Lord of the Rings: The Return of the King, The (2003)']\n",
            "['Lord of the Rings: The Return of the King, The (2003)']--> ['Lord of the Rings: The Two Towers, The (2002)']\n",
            "['Apollo 13 (1995)']--> ['Forrest Gump (1994)']\n",
            "['Pulp Fiction (1994)', 'Silence of the Lambs, The (1991)']--> ['Shawshank Redemption, The (1994)']\n",
            "['Silence of the Lambs, The (1991)', 'Shawshank Redemption, The (1994)']--> ['Pulp Fiction (1994)']\n"
          ]
        }
      ],
      "source": [
        "movie = {}\n",
        "size=len(df_movie)\n",
        "for i in range(size):\n",
        "  temp_k=df_movie.loc[i,'movieId']\n",
        "  val=df_movie.loc[i,'title']\n",
        "  movie[temp_k] =val\n",
        "\n",
        "left = []\n",
        "right = []\n",
        "for i in association_rules:\n",
        "    left.insert(len(left),i[0])\n",
        "    right.insert(len(right),i[1])\n",
        "association_rules_left = []\n",
        "for i in left:\n",
        "    t_array = []\n",
        "    for j in range(len(i)):\n",
        "        temp_k = i[j]\n",
        "        t_array.insert(len(t_array),movie[(temp_k)])\n",
        "        rule_cond = movie[(temp_k)]\n",
        "    association_rules_left.insert(len(association_rules_left),t_array)\n",
        "\n",
        "association_rules_right = []\n",
        "for i in right:\n",
        "    t_array = []\n",
        "    for j in range(0,len(i)):\n",
        "        temp_k = i[j]\n",
        "        rule_cond = movie[(temp_k)]\n",
        "        t_array.insert(len(t_array),rule_cond)\n",
        "    association_rules_right.insert(len(association_rules_right),t_array)\n",
        "\n",
        "association_rules_final=[]\n",
        "for x in range(len(association_rules_left)):\n",
        "    association_rules_final.insert(len(association_rules_final),list([association_rules_left[x],association_rules_right[x]]))\n",
        "print(\"There are\",len(association_rules),\"Association rules and they are as follows \\n\")\n",
        "\n",
        "for x in association_rules_final:\n",
        "    print(str(x[0])+\"--> \"+str((x[1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfeufQAxNB82"
      },
      "source": [
        "### (15pts) Step 3: Implement Random Sampling\n",
        "\n",
        "We discussed in class a method to randomly sample baskets to avoid the overhead of reading the entire set of baskets (which in practice, could amount to billions of baskets). For this part, you should implement such a random sampling approach that takes a special parameter **alpha** that controls the size of the sample: e.g., alpha = 0.10 means to sample 10% of the baskets (our users, in this case). \n",
        "\n",
        "Vary **alpha** and report the number of frequent itemsets you find and how this compares to the number of frequent itemsets in the entire dataset. What do you discover?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bslz87rc9kET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7baca6-9bd1-4fbe-f3a8-8c3a24acf9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Group the data on User Ids\n",
            "0      [3, 1023, 3168, 2628, 2291, 2137, 2899, 1092, ...\n",
            "1                                [115713, 74458, 109487]\n",
            "2                                                 [5181]\n",
            "3      [1188, 4896, 2874, 1500, 2762, 538, 176, 3386,...\n",
            "4                                      [58, 36, 50, 110]\n",
            "                             ...                        \n",
            "587    [5218, 5550, 1734, 431, 33903, 6350, 994, 3005...\n",
            "588    [2006, 2858, 3421, 2907, 5060, 457, 553, 1997,...\n",
            "589    [8784, 2194, 44665, 533, 34319, 37386, 802, 34...\n",
            "590                        [339, 116, 296, 1150, 288, 1]\n",
            "591    [54785, 91630, 7894, 140174, 94867, 79702, 162...\n",
            "Name: movieId, Length: 592, dtype: object\n",
            "\n",
            "Group the data on User Ids\n",
            "0      [3386, 1282, 1136, 1196, 110, 593, 943, 3441, ...\n",
            "1      [46970, 86345, 74458, 318, 68157, 79132, 12288...\n",
            "2                              [26409, 3024, 2288, 7991]\n",
            "3      [2762, 2174, 904, 2390, 1500, 1265, 1883, 215,...\n",
            "4      [265, 253, 232, 595, 349, 21, 597, 318, 527, 5...\n",
            "                             ...                        \n",
            "604    [47894, 1306, 56333, 45720, 4306, 2019, 8813, ...\n",
            "605    [1625, 2421, 165, 3793, 1552, 1518, 2004, 3273...\n",
            "606    [2194, 3977, 3863, 4873, 6378, 3484, 533, 4367...\n",
            "607    [318, 786, 434, 650, 296, 231, 10, 1059, 116, ...\n",
            "608    [71156, 27491, 7293, 149011, 138036, 3879, 110...\n",
            "Name: movieId, Length: 609, dtype: object\n",
            "\n",
            "Group the data on User Ids\n",
            "0      [1275, 1644, 2899, 441, 1127, 1030, 1552, 1032...\n",
            "1      [112552, 46970, 106782, 109487, 80489, 131724,...\n",
            "2      [849, 3703, 1371, 5919, 5764, 2288, 7899, 6835...\n",
            "3      [21, 3317, 1914, 4881, 1923, 1086, 593, 4121, ...\n",
            "4      [110, 588, 595, 39, 261, 589, 300, 531, 475, 2...\n",
            "                             ...                        \n",
            "604    [5304, 444, 461, 2360, 1094, 2387, 3408, 2431,...\n",
            "605    [2710, 1207, 2797, 3755, 2420, 3684, 3017, 200...\n",
            "606    [1393, 4370, 924, 3005, 1792, 3300, 8966, 780,...\n",
            "607    [1161, 454, 292, 480, 1059, 833, 892, 110, 137...\n",
            "608    [8983, 128360, 4728, 42718, 79702, 1721, 593, ...\n",
            "Name: movieId, Length: 609, dtype: object\n",
            "\n",
            "\n",
            "Total of  3073 items.\n",
            "\n",
            "1 Itemsets with support > 15.0 are 43\n",
            "\n",
            "{1: {frozenset({1580}), frozenset({32}), frozenset({10}), frozenset({1}), frozenset({380}), frozenset({1196}), frozenset({150}), frozenset({2959}), frozenset({1214}), frozenset({1704}), frozenset({480}), frozenset({39}), frozenset({4226}), frozenset({1210}), frozenset({1270}), frozenset({4993}), frozenset({3578}), frozenset({1198}), frozenset({2858}), frozenset({58559}), frozenset({1097}), frozenset({377}), frozenset({260}), frozenset({593}), frozenset({5952}), frozenset({318}), frozenset({4963}), frozenset({1089}), frozenset({2028}), frozenset({356}), frozenset({648}), frozenset({595}), frozenset({367}), frozenset({296}), frozenset({50}), frozenset({1036}), frozenset({110}), frozenset({34}), frozenset({165}), frozenset({527}), frozenset({2762}), frozenset({2329}), frozenset({2571})}}\n",
            "\n",
            "Total of  6459 items.\n",
            "\n",
            "1 Itemsets with support > 75.0 are 39\n",
            "\n",
            "{1: {frozenset({32}), frozenset({1}), frozenset({380}), frozenset({150}), frozenset({1196}), frozenset({2959}), frozenset({480}), frozenset({7153}), frozenset({4226}), frozenset({1270}), frozenset({1210}), frozenset({4993}), frozenset({1198}), frozenset({4306}), frozenset({58559}), frozenset({2858}), frozenset({780}), frozenset({457}), frozenset({589}), frozenset({47}), frozenset({858}), frozenset({260}), frozenset({593}), frozenset({5952}), frozenset({588}), frozenset({592}), frozenset({364}), frozenset({318}), frozenset({2028}), frozenset({356}), frozenset({1089}), frozenset({296}), frozenset({50}), frozenset({590}), frozenset({110}), frozenset({527}), frozenset({2762}), frozenset({608}), frozenset({2571})}}\n",
            "\n",
            "Total of  7359 items.\n",
            "\n",
            "1 Itemsets with support > 105.0 are 36\n",
            "\n",
            "\n",
            "2 Itemsets with support > 105.0 are 1\n",
            "\n",
            "{1: {frozenset({32}), frozenset({1}), frozenset({380}), frozenset({150}), frozenset({1196}), frozenset({2959}), frozenset({480}), frozenset({7153}), frozenset({1270}), frozenset({1210}), frozenset({4993}), frozenset({3578}), frozenset({1198}), frozenset({4306}), frozenset({58559}), frozenset({2858}), frozenset({457}), frozenset({589}), frozenset({47}), frozenset({858}), frozenset({260}), frozenset({593}), frozenset({5952}), frozenset({588}), frozenset({592}), frozenset({364}), frozenset({318}), frozenset({2028}), frozenset({356}), frozenset({296}), frozenset({50}), frozenset({110}), frozenset({527}), frozenset({2762}), frozenset({608}), frozenset({2571})}, 2: {frozenset({356, 318})}}\n",
            "69\n",
            "43\n",
            "39\n",
            "37\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "import random\n",
        "\n",
        "ML1 = mappingdata(0.1)\n",
        "ML2 = mappingdata(0.5)\n",
        "ML3 = mappingdata(0.7)\n",
        "\n",
        "r1, tf1 = apriori(ML1, 150, 0.9, 0.1, 1)\n",
        "r2, tf2 = apriori(ML2, 150, 0.9, 0.5, 1)\n",
        "r3, tf3 = apriori(ML3, 150, 0.9, 0.7, 1)\n",
        "\n",
        "\n",
        "tf1 = list(tf1.values())\n",
        "tf2 = list(tf2.values())\n",
        "tf3 = list(tf3.values())\n",
        "L1 = []\n",
        "for i in tf1:\n",
        "  L1 = L1 + list(i)\n",
        "\n",
        "L2 = []\n",
        "for i in tf2:\n",
        "  L2 = L2 + list(i)\n",
        "\n",
        "L3 = []\n",
        "for i in tf3:\n",
        "  L3 = L3 + list(i)\n",
        "\n",
        "print(len(total_movie_List))\n",
        "print(len(L1))\n",
        "print(len(L2))\n",
        "print(len(L3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIBMkM-W-8Tn"
      },
      "source": [
        "*your discussion here*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*  **By increasing the alpha, frequent itemsets are reducing further**\n",
        "\n",
        "\n",
        "1.   There are 69 frequent itemsets for the entire dataset\n",
        "2.   By Sampling 10% Dataset with minsup 15, 43 frequent itemsets are achieved\n",
        "3.   By Sampling 50% Dataset with minsup 75, 39 frequent itemsets are achieved\n",
        "4.   By Sampling 70% Dataset with minsup 105, 37 frequent itemsets are achieved\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLNDiFDrI2-2"
      },
      "source": [
        "### (10pts) Step 4: Check for False Positives\n",
        "\n",
        "Next you should verify that the candidate pairs you discover by random sampling are truly frequent by comparing to the itemsets you discover over the entire dataset. \n",
        "\n",
        "For this part, consider another parameter **minsup_sample** that relaxes the minimum support threshold. For example if we want minsup = 1/100 for whole dataset, then try minsup_sample = 1/125 for the sample. This will help catch truly frequent itemsets.\n",
        "\n",
        "Vary **minsup_sample** and report the number of frequent itemsets you find and the number of false positives you find. What do you discover?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8gtIPuf_82B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0165237-bc8d-4f40-880e-927106370dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total of  3073 items.\n",
            "\n",
            "1 Itemsets with support > 10 are 129\n",
            "\n",
            "{1: {frozenset({292}), frozenset({1304}), frozenset({32}), frozenset({10}), frozenset({316}), frozenset({1196}), frozenset({1961}), frozenset({4995}), frozenset({51662}), frozenset({1704}), frozenset({924}), frozenset({50872}), frozenset({48780}), frozenset({6539}), frozenset({457}), frozenset({5418}), frozenset({47}), frozenset({858}), frozenset({593}), frozenset({5952}), frozenset({588}), frozenset({5445}), frozenset({595}), frozenset({367}), frozenset({110}), frozenset({34}), frozenset({2762}), frozenset({3}), frozenset({293}), frozenset({1387}), frozenset({2571}), frozenset({4973}), frozenset({1258}), frozenset({8874}), frozenset({25}), frozenset({111}), frozenset({1214}), frozenset({3996}), frozenset({551}), frozenset({4226}), frozenset({1682}), frozenset({1307}), frozenset({1721}), frozenset({541}), frozenset({4993}), frozenset({8360}), frozenset({58559}), frozenset({2858}), frozenset({6874}), frozenset({68954}), frozenset({89745}), frozenset({377}), frozenset({260}), frozenset({592}), frozenset({356}), frozenset({4886}), frozenset({750}), frozenset({296}), frozenset({1136}), frozenset({50}), frozenset({7438}), frozenset({1221}), frozenset({1199}), frozenset({7361}), frozenset({1240}), frozenset({1265}), frozenset({1193}), frozenset({1580}), frozenset({231}), frozenset({40815}), frozenset({1}), frozenset({380}), frozenset({150}), frozenset({5218}), frozenset({586}), frozenset({1291}), frozenset({480}), frozenset({5060}), frozenset({300}), frozenset({7153}), frozenset({39}), frozenset({736}), frozenset({48516}), frozenset({8665}), frozenset({3578}), frozenset({5378}), frozenset({1225}), frozenset({79132}), frozenset({1097}), frozenset({5618}), frozenset({253}), frozenset({318}), frozenset({1089}), frozenset({648}), frozenset({2194}), frozenset({3949}), frozenset({1036}), frozenset({165}), frozenset({2000}), frozenset({6377}), frozenset({6}), frozenset({608}), frozenset({1208}), frozenset({6365}), frozenset({1206}), frozenset({55820}), frozenset({1222}), frozenset({2959}), frozenset({2291}), frozenset({1197}), frozenset({1210}), frozenset({1270}), frozenset({3793}), frozenset({1198}), frozenset({780}), frozenset({589}), frozenset({2987}), frozenset({72998}), frozenset({364}), frozenset({4963}), frozenset({2502}), frozenset({2028}), frozenset({2916}), frozenset({349}), frozenset({2918}), frozenset({527}), frozenset({2716}), frozenset({2329}), frozenset({1213})}}\n",
            "\n",
            "Total of  3073 items.\n",
            "\n",
            "1 Itemsets with support > 13 are 66\n",
            "\n",
            "{1: {frozenset({1580}), frozenset({32}), frozenset({1258}), frozenset({25}), frozenset({10}), frozenset({1}), frozenset({380}), frozenset({1196}), frozenset({150}), frozenset({2959}), frozenset({1214}), frozenset({5218}), frozenset({1197}), frozenset({1704}), frozenset({586}), frozenset({480}), frozenset({7153}), frozenset({39}), frozenset({4226}), frozenset({1213}), frozenset({1210}), frozenset({1270}), frozenset({4993}), frozenset({3578}), frozenset({79132}), frozenset({1198}), frozenset({2858}), frozenset({58559}), frozenset({6539}), frozenset({780}), frozenset({1097}), frozenset({5418}), frozenset({6874}), frozenset({2987}), frozenset({47}), frozenset({377}), frozenset({260}), frozenset({593}), frozenset({5952}), frozenset({588}), frozenset({253}), frozenset({318}), frozenset({4963}), frozenset({2028}), frozenset({1089}), frozenset({5445}), frozenset({356}), frozenset({648}), frozenset({4886}), frozenset({595}), frozenset({367}), frozenset({296}), frozenset({50}), frozenset({1036}), frozenset({110}), frozenset({34}), frozenset({165}), frozenset({527}), frozenset({1221}), frozenset({2762}), frozenset({6377}), frozenset({2716}), frozenset({1199}), frozenset({1208}), frozenset({2329}), frozenset({2571})}}\n",
            "\n",
            "Total of  3073 items.\n",
            "\n",
            "1 Itemsets with support > 15 are 43\n",
            "\n",
            "{1: {frozenset({1580}), frozenset({32}), frozenset({10}), frozenset({1}), frozenset({380}), frozenset({1196}), frozenset({150}), frozenset({2959}), frozenset({1214}), frozenset({1704}), frozenset({480}), frozenset({39}), frozenset({4226}), frozenset({1210}), frozenset({1270}), frozenset({4993}), frozenset({3578}), frozenset({1198}), frozenset({2858}), frozenset({58559}), frozenset({1097}), frozenset({377}), frozenset({260}), frozenset({593}), frozenset({5952}), frozenset({318}), frozenset({4963}), frozenset({1089}), frozenset({2028}), frozenset({356}), frozenset({648}), frozenset({595}), frozenset({367}), frozenset({296}), frozenset({50}), frozenset({1036}), frozenset({110}), frozenset({34}), frozenset({165}), frozenset({527}), frozenset({2762}), frozenset({2329}), frozenset({2571})}}\n",
            "10% Random sampling and minsup = 10 the count of false positive is 94\n",
            "50% Random sampling and minsup = 13 the count of false positive is 37\n",
            "70% Random sampling and minsup = 15 the count of false positive is18\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "r1, tf1 = apriori(ML1, 10, 0.8, 1, 1)\n",
        "r2, tf2 = apriori(ML1, 13, 0.8, 1, 1)\n",
        "r3, tf3 = apriori(ML1, 15, 0.8, 1, 1)\n",
        "\n",
        "tf1 = list(tf1.values())\n",
        "tf2 = list(tf2.values())\n",
        "tf3 = list(tf3.values())\n",
        "l1 = []\n",
        "for i in tf1:\n",
        "  l1 = l1 + list(i)\n",
        "\n",
        "l2 = []\n",
        "for i in tf2:\n",
        "  l2 = l2 + list(i)\n",
        "\n",
        "l3 = []\n",
        "for i in tf3:\n",
        "  l3 = l3 + list(i)\n",
        "\n",
        "count1 = 0\n",
        "count2 = 0\n",
        "count3 = 0\n",
        "\n",
        "for i in (total_movie_List):\n",
        "  if i in l1:\n",
        "    count1 = count1+1\n",
        "print('10% Random sampling and minsup = 10 the count of false positive is ' + str( len(l1)-count1))\n",
        "\n",
        "\n",
        "for i in (total_movie_List):\n",
        "  if i in l2:\n",
        "    count2 = count2+1\n",
        "print('50% Random sampling and minsup = 13 the count of false positive is ' + str( len(l2)-count2))\n",
        "\n",
        "for i in (total_movie_List):\n",
        "  if i in l3:\n",
        "    count3 = count3+1\n",
        "print('70% Random sampling and minsup = 15 the count of false positive is ' + str( len(l3)-count3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgpWjmXh_91h"
      },
      "source": [
        "*your discussion here*\n",
        "\n",
        "---\n",
        "**By Increasing the confidence, the count of false positives reduces**\n",
        "\n",
        "\n",
        "*   10% Random sampling and minsup = 10 the count of false positive is 94\n",
        "* 50% Random sampling and minsup = 13 the count of false positive is 37\n",
        "* 70% Random sampling and minsup = 15 the count of false positive is 18\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt4JCrwcAHal"
      },
      "source": [
        "### (5pts) Step 5: Extensions and Next Steps\n",
        "\n",
        "So far, we have been working with a fairly small dataset. For this last question, try your sampling-based approach on the much larger: **Movies 10M** dataset: https://files.grouplens.org/datasets/movielens/ml-10m.zip\n",
        "\n",
        "First, we need to load this larger dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG7qBkj7AVou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1623c8fa-732a-48ce-dd88-f551a0e6f940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        }
      ],
      "source": [
        "import urllib3\n",
        "import zipfile\n",
        "\n",
        "http = urllib3.PoolManager()\n",
        "req = http.request(\"GET\", \"https://files.grouplens.org/datasets/movielens/ml-10m.zip\", preload_content=False)\n",
        "\n",
        "with open(\"movie.zip\", 'wb') as out:\n",
        "  while True:\n",
        "    data = req.read(4096)\n",
        "    if not data:\n",
        "      break\n",
        "    out.write(data)\n",
        "req.release_conn()\n",
        "\n",
        "zFile = zipfile.ZipFile(\"movie.zip\", \"r\")\n",
        "for fileM in zFile.namelist():\n",
        "  zFile.extract(fileM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hi45CqJht7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "163fa40e-9bb5-4804-b2c4-0a6feea06d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "allbut.pl  movies.dat  ratings.dat  README.html  split_ratings.sh  tags.dat\n"
          ]
        }
      ],
      "source": [
        "! ls ml-10M100K/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_jdR72WiR2F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# read user ratings\n",
        "allRatings = pd.read_csv(\"ml-10M100K/ratings.dat\",sep='::', names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"], engine='python')[allRatings['rating']>=3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEX9wu7ewIqb"
      },
      "source": [
        "Now you can begin your sampling over this larger dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYRlIEyulq2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7a9c84-bb8b-47f9-891f-539d2151880e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total of  2631 items.\n",
            "\n",
            "1 Itemsets with support > 10.0 are 172\n",
            "\n",
            "{1: {frozenset({292}), frozenset({1653}), frozenset({1304}), frozenset({32}), frozenset({1230}), frozenset({10}), frozenset({316}), frozenset({1196}), frozenset({539}), frozenset({1961}), frozenset({1639}), frozenset({924}), frozenset({1120}), frozenset({1259}), frozenset({587}), frozenset({4306}), frozenset({3481}), frozenset({457}), frozenset({2396}), frozenset({2006}), frozenset({47}), frozenset({858}), frozenset({593}), frozenset({5952}), frozenset({588}), frozenset({196}), frozenset({185}), frozenset({5445}), frozenset({1617}), frozenset({595}), frozenset({2}), frozenset({367}), frozenset({6333}), frozenset({2012}), frozenset({590}), frozenset({110}), frozenset({34}), frozenset({2762}), frozenset({293}), frozenset({5349}), frozenset({1387}), frozenset({2571}), frozenset({628}), frozenset({4973}), frozenset({1258}), frozenset({153}), frozenset({25}), frozenset({1246}), frozenset({1552}), frozenset({1214}), frozenset({3527}), frozenset({3996}), frozenset({1391}), frozenset({161}), frozenset({4226}), frozenset({3081}), frozenset({1200}), frozenset({1307}), frozenset({2174}), frozenset({1721}), frozenset({541}), frozenset({4993}), frozenset({2947}), frozenset({2858}), frozenset({454}), frozenset({1101}), frozenset({344}), frozenset({1356}), frozenset({2100}), frozenset({969}), frozenset({377}), frozenset({260}), frozenset({913}), frozenset({592}), frozenset({356}), frozenset({4886}), frozenset({750}), frozenset({296}), frozenset({1073}), frozenset({50}), frozenset({1136}), frozenset({5669}), frozenset({1221}), frozenset({329}), frozenset({1240}), frozenset({2683}), frozenset({62}), frozenset({1193}), frozenset({1265}), frozenset({1580}), frozenset({2302}), frozenset({223}), frozenset({434}), frozenset({474}), frozenset({1}), frozenset({380}), frozenset({1247}), frozenset({150}), frozenset({1092}), frozenset({1291}), frozenset({733}), frozenset({480}), frozenset({300}), frozenset({7153}), frozenset({39}), frozenset({736}), frozenset({919}), frozenset({410}), frozenset({3578}), frozenset({1225}), frozenset({500}), frozenset({17}), frozenset({1097}), frozenset({832}), frozenset({597}), frozenset({253}), frozenset({442}), frozenset({318}), frozenset({1089}), frozenset({1968}), frozenset({648}), frozenset({1370}), frozenset({2194}), frozenset({4014}), frozenset({3949}), frozenset({1036}), frozenset({165}), frozenset({2000}), frozenset({6377}), frozenset({3114}), frozenset({608}), frozenset({1208}), frozenset({339}), frozenset({357}), frozenset({1206}), frozenset({1584}), frozenset({1220}), frozenset({3147}), frozenset({4022}), frozenset({509}), frozenset({36}), frozenset({235}), frozenset({2959}), frozenset({2291}), frozenset({1197}), frozenset({1393}), frozenset({350}), frozenset({440}), frozenset({1210}), frozenset({1270}), frozenset({141}), frozenset({2700}), frozenset({3793}), frozenset({1198}), frozenset({780}), frozenset({11}), frozenset({589}), frozenset({2987}), frozenset({2997}), frozenset({923}), frozenset({364}), frozenset({2502}), frozenset({4963}), frozenset({2028}), frozenset({2916}), frozenset({95}), frozenset({349}), frozenset({2918}), frozenset({527}), frozenset({2716}), frozenset({1127}), frozenset({2329})}}\n",
            "172\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "df_movie_rating = allRatings.sample(frac = 0.001)\n",
        "\n",
        "group_ID = df_movie_rating.groupby('userId')\n",
        "M_Id=[]\n",
        "for i in group_ID.groups.keys():\n",
        "    M_Id.append((group_ID.get_group(i).movieId.values))\n",
        "\n",
        "M_L=pd.DataFrame({'userId':group_ID.groups.keys(),'M_Id':M_Id}).M_Id\n",
        "\n",
        "rules, total_Frequency = apriori(M_L, 10000, 0.9, 0.001, 1)\n",
        "\n",
        "tlist = []\n",
        "for i in list(total_Frequency.values()):\n",
        "  tlist = tlist + list(i)\n",
        "\n",
        "print(len(tlist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX3LDkfAlpyg"
      },
      "source": [
        "*your discussion here*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "By Sampling data for 0.001% and factored by 10000 support value, the frequent itemset calculated was 172"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "431001750_hw1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}